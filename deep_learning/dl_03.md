# Deep learning: a critical appraisal
Gary Marcus (2018)

## Key points
- DL needs large amounts of data to generalize well, and test must be similar to train (allowing interpolation)
    - Generalization = interpolation + extrapolation (beyond known examples)
- 10 challenges for DL:
    1. DL is data-hungry: DL lacks mechanism to learn abstractions through explicit/implicit definition, instead needs lots of data
    2. DL has problems learning robust and abstract concepts, and then transferring these to different scenarios
    3. DL has no natural way of dealing with hierarchy, making generalization hard for tasks where this is important
        - What about CNNs? They do have this!
    4. DL struggles with open-ended inference (inference beyond what is in text: multiple sentences or sentences + background knowledge)
    5. DL is a black box: problem for debuggability and understanding how decisions were made (medical field)
    6. DL is self-contained and isolated from other useful knowledge, and this is hard to incorporate
        - DL uses feature correlations instead of abstractions
    7. DL cannot inherently distinguish correlation from causation
    8. DL presumes a largely stable world
    9. DL can be easily fooled: quite good at some large fraction of a domain, but easily spoofed in some spot (adversarial examples)
    10. DL systems are difficult to expand while guaranteeing performance --> hard to engineer with!
- So: DL much better at interpolation than extrapolation
- See DL as a tool in a toolbox
- Better/more complete techniques:
    - Unsupervised --> children also learn like this!
    - Symbolic AI + DL: incorporate knowledge, allowing for inference and abstraction
- Look at human cognition
- Go beyond the Turing test: comprehension, scientific reasoning, IKEA-set-building, game-playing as challenges
