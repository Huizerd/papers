# Large-scale machine learning with stochastic gradient descent
LÃ©on Bottou (2010)

## Key points
- Computing time is limiting, not sample size
    - SGD shows great performance for large-scale problems (lots of data)
    - SGD here: 1 sample (usually, n samples)
- Small-scale datasets: constrained by data --> use GD
- Large-scale datasets: constrained by time --> use SGD
- Error = optimization error (reduce by optimizing longer) + approximation error (reduce by choosing larger function family) + estimation error (smaller function family or more data)
- Although SGD is the worst optimization algorithm tested here, it needs the least amount of time to reach a specified performance --> good for large-scale problems!
- SGD:
    - Strengths: efficient updates, intuitive
    - Weaknesses: many assumptions, high variance
