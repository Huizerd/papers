# Residual networks behave like ensembles of relatively shallow networks
Andreas Veit, Michael Wilber, Serge Belongie (2016)

## Key points
- Residual networks can be seen as a collection of many paths of differing length
    - Each goes through a different subset of layers
    - Show ensemble-like behavior (not dependent on each other)
- The only effective paths are relatively shallow
    - Enable very deep networks: resolve the vanishing gradient problem by using the short paths to carry the gradient (instead of the view that this is achieved through preserving gradient flow throughout the entire depth)
    - Only short paths needed during training
- At test time: removing single layer doesn't affect performance (unlike VGG!)
    - Remove residual module, leave skip connection + downsampling
    - Removal mostly affects long paths, which are not "needed"!
    - Negligible loss in performance for < 10 modules
- More layers still make residual networks better: more valid paths = better performance
- Residual networks can be reconfigured to some extent at runtime
- Distribution of all possible path lengths is binomial
- Highway networks: also reinforce importance of short paths
