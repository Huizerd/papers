# On random weights and unsupervised feature learning
Andrew M. Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, Andrew Y. Ng (2010)

## Key points
- Certain convolutional pooling architectures can be inherently frequency selective and translation invariant, even with random weights
    - Goes for circular convolution, however: circular convolution = valid convolution for image size --> infinity
    - Random input doesn't lead to random output
    - Frequencies = features in different domains
    - So, random filters may be good discriminators of images
    - Distribution of random weights doesn't matter!
    - Same principle applies to non-convolutional architectures
- Contribution of pre-training + fine tuning < contribution of architecture
- Fast architecture selection:
    - Random-weight performance correlates with trained performance
    - NOTE: use multiple realizations for evaluations, since the difference in performance between realizations is not negligible!
- Future papers should report the performance of architecture with random weights, to see the training improvement
