# Curriculum dropout
Pietro Morerio, Jacopo Cavazza, Riccardo Volpi, Ren√© Vidal, Vittorio Murino (2017)

## Key points
- Dropout: prevents overfitting and improves regularization
    - Can be seen as model aggregation: different network for each sample
    - Distribute responsibility between neurons
- However: overfitting is unlikely to occur during the early stages of training
- Curriculum dropout: dynamically increase suppressed neurons
    - Start easy (from curriculum learning)
    - Improve generalization
- Curriculum dropout can be seen as:
    - Slightly smarter weight initialization + regular dropout
    - Gradually adding noise to the input and intermediate representations
- Little effect for simple tasks (no need to start easy)
- No data augmentation/pre-processing needed!
- Dropout fails for thin networks (chance of a "gap")
