# Structured receptive fields in CNNs
JÃ¶rn-Henrik Jacobsen, Jan van Gemert, Zhongyou Lou, Arnold W.M. Smeulders (2016)

## Key points
- Learn good representation with limited data: pre-training or incorporating knowledge
    - Combine both into structured receptive field networks
- Fixed filter base, but with the flexibility of a CNN
    - Receptive fields are a weighted sum over a fixed basis, so only the weights have to be learned
    - Learn arbitrary effective filters from a basis rather than modelling them
- Assumes all images are spatially coherent --> no need to learn this!
- Similar to scattering networks (SNs): which have fixed filter parameters
    - Best of both worlds: better than CNNs on small datasets, better than SNs on large datasets
- Aim for a CNN that treats images as functions in scale-space
    - Allows us to use differential operators on images
- The basis of filters is a complete and minimal set (guaranteed by the 4th-order Taylor series and the Gaussian derivatives)
    - Receptive field neural network (RFNN): CNN when a complete polynomial up to order infinity is considered --> we restrict ourselves to order <= 4 (anything above doesn't carry meaningful visual information)
        - Type of regularization!
        - Decreases the number of filters in a CNN from 520,000 to 12-144!!
- Steerability: arbitrary rotations from basis rather than rotating the basis itself --> SNs are a special case of RFNN
- Learn Gaussian derivative basis function weights instead of kernels
    - You can learn only smooth functions: again sort of regularization, since the world is smooth (so noise can't be learned)
- Allows CNNs to be combined with all kinds of multiscale image analysis research!
- Convolving with effective filter = convolving with basis and then recombining feature maps (can't actually compute effective filters)
