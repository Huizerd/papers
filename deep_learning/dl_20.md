# Visualizing and understanding recurrent networks
Andrej Karpathy, Justin Johnson, Li Fei-Fei (2015)

## Key points
- Exploration of LSTMs using character-level language models --> reveal of "cells with a purpose" (e.g. line endings)
- Comparison to n-gram models, which predict the next letter from previous ones using a probabilistic model
    - Superiority of LSTMs in long-range reasoning
    - But, also LSTMs have some problems with long-range reasoning, probably due to SGD dynamics, truncated backprop
- Error analysis of LSTMs: shows that further architectural innovations are needed instead of simply scaling up
- Gated recurrent unit (GRU): simpler than LSTM, no separate hidden and memory cell + 3 gates instead of 4
